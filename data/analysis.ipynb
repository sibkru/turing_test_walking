{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ~~Importing more than one json file~~\n",
    "- ~~How to group by subjects models and movement~~\n",
    "- ~~Model specific confusion rat~~\n",
    "- How to manage or represent all this data\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping\n",
    "- subjects\n",
    "- models\n",
    "    - type overall\n",
    "    - individual configuration\n",
    "- movement\n",
    "    - type overall\n",
    "    - with(out) contact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data\n",
    "json file needs to be normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_modelname(x='model(dmp)-dataset(return-bottle-hold)-npsi(90)-hold(1)'):\n",
    "    x = x.split('-dataset(')\n",
    "    typ = x[1].split(')',1)\n",
    "    typ = typ[1]\n",
    "    x = x[0].split('model(')\n",
    "    x = x[1][:-1]\n",
    "    return x + typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_json = '/users/hieu/git/turing_test_walking/data'\n",
    "\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "json_files.pop()\n",
    "first_file = json_files.pop()\n",
    "with open(first_file,'r') as f:\n",
    "    one = json.loads(f.read())\n",
    "data = pd.json_normalize(one)\n",
    "\n",
    "#workaround for concatenating unnormalized data\n",
    "for file in json_files:\n",
    "    with open(file,'r') as f:\n",
    "        one = json.loads(f.read())\n",
    "    load = pd.json_normalize(one)\n",
    "    data = pd.concat([data,load])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering test parts and creating new index\n",
    "df = data[data['test_part'] == 'response'].reset_index()\n",
    "df = df.fillna(value='99')\n",
    "df = df[df['key_press'] != '99'].reset_index()\n",
    "\n",
    "# this line below for filtering attention checks overall out -> uncomment for using it\n",
    "# df = data[(data['test_part'] == 'response') & (data['.att_check'] == False)].reset_index()\n",
    "\n",
    "# must be integers for comparing\n",
    "df['key_press'] = df['key_press'].astype(int)\n",
    "df['test_stimulus.position'] = df['test_stimulus.position'].astype(int)\n",
    "\n",
    "# renaming columns\n",
    "df = df.rename(columns={\"test_stimulus.left\": \"left\",\n",
    "                        \"test_stimulus.right\": \"right\",\n",
    "                        \"test_stimulus.att_check\": \"att_check\",\n",
    "                        \"test_stimulus.position\": \"position\",\n",
    "                        \"key_press\":\"pressed\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['sona','subject','rt','att_check','pressed','position','left','right']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# without copy() there is an error when replacing strings\n",
    "d = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecessary strings\n",
    "d['left'] = d['left'].str.replace('bvh/vr_prediction_models/','')\n",
    "d['left'] = d['left'].str.replace('bvh/','')\n",
    "d['left'] = d['left'].str.replace('/final-lines.txt','')\n",
    "d['left'] = d['left'].str.replace('-lines.txt','')\n",
    "d['right'] = d['right'].str.replace('bvh/vr_prediction_models/','')\n",
    "d['right'] = d['right'].str.replace('bvh/','')\n",
    "d['right'] = d['right'].str.replace('/final-lines.txt','')\n",
    "d['right'] = d['right'].str.replace('-lines.txt','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['pressed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode keypress\n",
    "num = 0\n",
    "arr = []\n",
    "for i in d['pressed']:\n",
    "    if i == 70:\n",
    "        arr.append(0)\n",
    "        \n",
    "    elif i == 74:\n",
    "        arr.append(1)\n",
    "d['pressed'] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding movement column for better grouping\n",
    "num = 0\n",
    "arr = []\n",
    "\n",
    "for i in d['position']:\n",
    "    if i == 1:\n",
    "        arr.append(d['right'][num])\n",
    "    else:\n",
    "        arr.append(d['left'][num])\n",
    "    num += 1\n",
    "\n",
    "movement = []\n",
    "for i in arr:\n",
    "    x = i.split('-')\n",
    "    movement.append('-'.join(x[:-1]))\n",
    "d['movement'] = movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding model column for easier grouping by\n",
    "num = 0\n",
    "arr = []\n",
    "for i in d['position']:\n",
    "    if i == 1:\n",
    "        arr.append(d['left'][num])\n",
    "    else:\n",
    "        arr.append(d['right'][num])\n",
    "    num += 1\n",
    "d['model_set'] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**old code for model column**\n",
    "\n",
    "arr = []\n",
    "for i in d['model_set']:\n",
    "    string = i.split('-')\n",
    "    string = string[0].replace('model(','')\n",
    "    string = string.replace(')','')\n",
    "    arr.append(string)\n",
    "d['model'] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model column for better grouping by models\n",
    "arr = []\n",
    "for i in d['model_set']:\n",
    "    if i.find('tmp')>0:\n",
    "        arr.append('tmp')\n",
    "    elif i.find('dmp')>0:\n",
    "        arr.append('dmp')\n",
    "    elif i.find('MAP')>0:\n",
    "        arr.append('vcgpdm-map')\n",
    "    elif i.find('ELBO')>0:\n",
    "        arr.append('vcgpdm-elbo')\n",
    "        \n",
    "d['model'] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in d['model_set']:\n",
    "    arr.append(parse_modelname(i))\n",
    "d['model_set'] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding performance score\n",
    "num = 0\n",
    "arr = []\n",
    "for i in d['pressed']:\n",
    "    if i == d['position'][num]:\n",
    "        arr.append(1)\n",
    "    else:\n",
    "        arr.append(0)\n",
    "    num += 1\n",
    "d['p_score'] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking only Sona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d=d[d['sona']!='99']\n",
    "d=d[d['sona']!='THNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(d['sona'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis\n",
    "### overall confusion rate\n",
    "#### with all attention checks included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in d['p_score']:\n",
    "    correct += i\n",
    "confusion_rate = (len(d) - correct)/len(d)\n",
    "print(confusion_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without attention checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_check = d[d['att_check']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in no_check['p_score']:\n",
    "        correct += i\n",
    "confusion_rate = (len(no_check) - correct)/len(no_check)\n",
    "print(confusion_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_check.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking attention\n",
    "#### if confusion rate is zero -> good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_att = d[d['att_check']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in check_att['p_score']:\n",
    "        correct += i\n",
    "confusion_rate = (len(check_att) - correct)/len(check_att)\n",
    "print(confusion_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in d:\n",
    "        print(d[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(d['model']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "454 * 30 / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_check.groupby('model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = d.groupby('model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stringcolumn in *args (order of *args) \n",
    "#e.g.: compare_model_sets(d,'models','subject')\n",
    "\n",
    "def compare_model_sets(dataframe=d,*args): \n",
    "    #taking the sum of the values for dividing it later with the number of trials\n",
    "    comparison = dataframe.groupby(by=[*args]).sum()\n",
    "    ###print(comparison['p_score'])\n",
    "    \n",
    "    #counting the number of trials\n",
    "    num = dataframe.groupby(by=[*args]).count()\n",
    "    #print(num)\n",
    "    #assign the number of trials to the relevant df\n",
    "    comparison['num'] = num['p_score']\n",
    "    #computing confusion rate from \"p_score / num\"\n",
    "    #The confusion rate depends on how the grouping is put together\n",
    "    comparison['confusion_rate'] = 1 - (comparison['p_score'] / num['p_score'])\n",
    "    return comparison.reset_index()\n",
    "\n",
    "#compute confusion_rate manually\n",
    "def confusion_rate(p_score):\n",
    "    correct = 0\n",
    "    num = 0\n",
    "    for i in p_score:\n",
    "        correct += i\n",
    "        num += 1\n",
    "    return (num - correct)/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = d.groupby(d['model']).count()\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num1 = d.groupby(d['model_set']).count()\n",
    "num1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_model_sets(d,'model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_model_sets(d,'model_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmp = d[d['model']=='dmp']\n",
    "tmp = d[d['model']=='tmp']\n",
    "v_elbo = d[d['model']=='vcgpdm-elbo']\n",
    "v_map = d[d['model']=='vcgpdm-elbo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_rate(no_check['p_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby(by=['date', 'category']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compare_model_sets(d,'model','model_set')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compare_model_sets(d,'model_set','model')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['model_set'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model.pivot_table(values='confusion_rate',index='model',columns='model_set').fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
